<aticle class="text">
    <p>This projects implements the visualization of optimization loss for a
        linear multiclass classifier using Python and Flask. The idea is based
        on the <b>Multiclass Linear
            Classification Optimization Loss Visualization</b> <a
                href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/"
                target="_blank">web demo</a> by Justin Johnson. The text below is
        taken from that demo page as well.</p>
    <hr>
    <p>
        The class scores for linear classifiers are computed as
        \(f(x_i;W,b)=Wx_i+b\),
        where the parameters consist of weights \(W\) and biases \(b\). The
        training
        data is \(x_i\) with labels \(y_i\). In this demo, the datapoints
        \(x_i\) are
        2-dimensional and there are 3 classes, so the weight matrix is of size
        \([3 \times 2]\) and the bias vector is of size \([3 \times 1]\). The
        multiclass loss
        function can be formulated in many ways. The default in this demo is an
        SVM that follows [Weston and Watkins 1999]. Denoting \(f\) as the \([3
        \times 1]\)
        vector that holds the class scores, the loss has the form:
    </p>
    <p style="width:100%; overflow-x:auto">
        $$L= \underbrace{\frac{1}{N}\sum_{i=1}^{N}\sum_{j\neq y_i} max(0, f_j -
        f_{y_i} + 1)}_{\text{data loss}} + \underbrace{\sum_k\sum_l
        W_{k,l}^2}_{\text{regularization loss}}$$
    </p>
    <p>
        Where \(N\) is the number of examples, and \(Î»\) is a hyperparameter that
        controls the strength of the \(L2\) regularization penalty \(R(W)=\sum_k\sum_l W_{k, l}^2\).
        On the bottom right of this demo you can also flip to different
        formulations for the Multiclass SVM including One vs All (OVA) where a
        separate binary SVM is trained for every class independently (vs. other
        classes all labeled as negatives), and Structured SVM which maximizes
        the margin between the correct score and the score of the highest
        runner-up class. You can also choose to use the cross-entropy loss which
        is used by the Softmax classifier.
    </p>
</aticle>