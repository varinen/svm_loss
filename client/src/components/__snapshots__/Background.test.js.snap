// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`Background renders correctly 1`] = `
<MathJaxProvider
  options={
    Object {
      "showMathMenu": false,
      "showMathMenuMSIE": false,
      "tex2jax": Object {
        "inlineMath": Array [],
      },
    }
  }
  script="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"
>
  <div
    className="intro card shadow"
  >
    <div
      className="card-header py-3"
    >
      <h6
        className="m-0 font-weight-bold text-primary"
      >
        Background of the Demo
      </h6>
    </div>
    <div
      className="card-body"
    >
      <article
        className="text"
      >
        <p>
          This projects implements the visualization of optimization loss for a linear multiclass classifier using Python and Flask. The idea is based on the 
          <b>
            Multiclass Linear Classification Optimization Loss Visualization
          </b>
           
          <a
            href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/"
            rel="noopener noreferrer"
            target="_blank"
          >
            web demo
          </a>
           by Justin Johnson. The text below is taken from that demo page as well.
        </p>
        <hr />
        <p>
          The class scores for linear classifiers are computed as 
          <MathJaxNode
            formula="f(x_i;W,b)=Wx_i+b"
            inline={true}
          />
          , where the parameters consist of weights 
          <MathJaxNode
            formula="W"
            inline={true}
          />
           and biases 
          <MathJaxNode
            formula="b"
            inline={true}
          />
          . The training data is 
          <MathJaxNode
            formula="x_i"
            inline={true}
          />
          , with labels 
          <MathJaxNode
            formula="y_i"
            inline={true}
          />
          . In this demo, the data points 
          <MathJaxNode
            formula="x_i"
            inline={true}
          />
          , are 2-dimensional and there are 3 classes, so the weight matrix is of size 
          <MathJaxNode
            formula="[3 \\\\times 2]"
            inline={true}
          />
          , and the bias vector is of size 
          <MathJaxNode
            formula="[3 \\\\times 1]"
            inline={true}
          />
          . The multi-class loss function can be formulated in many ways. The default in this demo is an SVM that follows [Weston and Watkins 1999]. Denoting 
          <MathJaxNode
            formula="f"
            inline={true}
          />
          , as the 
          <MathJaxNode
            formula="[3 \\\\times 1]"
            inline={true}
          />
          , vector that holds the class scores, the loss has the form:
        </p>
        <div
          style={
            Object {
              "overflowX": "auto",
              "width": "100%",
            }
          }
        >
          <MathJaxNode
            formula="L= \\\\underbrace{\\\\frac{1}{N}\\\\sum_{i=1}^{N}\\\\sum_{j\\\\neq y_i} max(0, f_j -
        f_{y_i} + 1)}_{\\\\text{data loss}} + \\\\underbrace{\\\\sum_k\\\\sum_l
        W_{k,l}^2}_{\\\\text{regularization loss}}"
          />
        </div>
        <p>
          Where 
          <MathJaxNode
            formula="N"
            inline={true}
          />
           is the number of examples, and 
          <MathJaxNode
            formula="Î»"
            inline={true}
          />
           is a hyperparameter that controls the strength of the 
          <MathJaxNode
            formula="L2"
            inline={true}
          />
          regularization penalty 
          <MathJaxNode
            formula="R(W)=\\\\sum_k\\\\sum_l W_{k, l}^2"
            inline={true}
          />
          On the bottom right of this demo you can also flip to different formulations for the Multiclass SVM including One vs All (OVA) where a separate binary SVM is trained for every class independently (vs. other classes all labeled as negatives), and Structured SVM which maximizes the margin between the correct score and the score of the highest runner-up class. You can also choose to use the cross-entropy loss which is used by the Softmax classifier.
        </p>
      </article>
    </div>
  </div>
</MathJaxProvider>
`;
